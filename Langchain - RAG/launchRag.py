from langchain_community.document_loaders import CSVLoader, PyMuPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_community.vectorstores import SKLearnVectorStore
from sentence_transformers import SentenceTransformer # https://www.sbert.net/index.html
from langchain.embeddings.base import Embeddings

from langchain_ollama import ChatOllama
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain.schema import Document

import os

# Chemin de sauvegarde du VectorStore
PERSIST_PATH = os.path.join('./Store/', "vectorstore.pqt")

active_log = False # Activer/d√©sactiver l'affichage des logs

def log(message):
    if active_log:
        print(message)

def startRag():
    print("üîÑ Chargement du RAG...")

    # Charger le mod√®le d'embedding
    model = SentenceTransformer("all-MiniLM-L6-v2")

    # Fonction de r√©cup√©ration des embeddings
    class PrecomputedEmbeddings(Embeddings):
        def __init__(self, precomputed_vectors):
            self.precomputed_vectors = precomputed_vectors  # Stocke les embeddings calcul√©s
    
        def embed_documents(self, texts):
            return self.precomputed_vectors # Retourne les embeddings d√©j√† calcul√©s.
    
        def embed_query(self, text):
            return model.encode([text])[0]  # G√©n√®re l'embedding pour une nouvelle requ√™te et retourne un seul vecteur

    # V√©rifier si le VectorStore existe d√©j√†
    if os.path.exists(PERSIST_PATH):   
        # Charger le VectorStore existant
        vectorstore = SKLearnVectorStore(
            persist_path=PERSIST_PATH,
            embedding=PrecomputedEmbeddings([]),  # Embeddings fictifs pour respecter l'interface
            serializer="parquet"
        )

        # Cr√©er un retriver
        retriever = vectorstore.as_retriever(k=1)

        ### Retrieval Grader
        llm = ChatOllama(
            model="mistral:latest",
            format="json",
            temperature=0,
        )

        prompt = PromptTemplate(
            template="""Tu es un √©valuateur √©valuant la pertinence d'un document r√©cup√©r√© par rapport √† une question d'un utilisateur. \n 
            Voici le document r√©cup√©r√© : \n\n {document} \n\n
            Voici la question de l'utilisateur : {question} \n
            Si le document contient des mots-cl√©s li√©s √† la question de l'utilisateur, note-le comme pertinent. \n
            Ce n'est pas n√©cessaire que ce soit un test rigoureux. Le but est de filtrer les r√©cup√©rations erron√©es. \n
            Donne un score binaire ¬´ oui ¬ª ou ¬´ non ¬ª pour indiquer si le document est pertinent par rapport √† la question. \n
            Fournis le score binaire au format JSON avec une cl√© unique ¬´ score ¬ª et sans pr√©ambule ni explication.""",
            input_variables=["question", "document"],
        )

        retrieval_grader = prompt | llm | JsonOutputParser()
        question = "agent memory"
        docs = retriever.invoke(question)
        doc_txt = docs[1].page_content

        ### Generate
        llm = ChatOllama(
            model="mistral:latest",
            temperature=0,
        )

        prompt = PromptTemplate(
            template="""Utilise les documents suivants pour r√©pondre √† la question.
            Ne donne pas de r√©ponse sans utiliser les documents.
            Tu devras obligatoirement r√©pondre en fran√ßais.
            Question: {question}
            Documents: {documents}
            R√©ponse:
            """,
            input_variables=["question", "documents"],
        )

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        rag_chain = prompt | llm | StrOutputParser()
        generation = rag_chain.invoke({"documents": docs, "question": question})

        ### Hallucination Grader
        llm = ChatOllama(
            model="mistral:latest",
            format="json",
            temperature=0,
        )

        prompt = PromptTemplate(
            template="""Tu es un √©valuateur qui √©value si une r√©ponse est fond√©e/√©tay√©e par un ensemble de faits. \n 
            Voici les faits :
            \n ------- \n
            {documents} 
            \n ------- \n
            Voici la r√©ponse : {generation}
            Donne un score binaire ¬´ oui ¬ª ou ¬´ non ¬ª pour indiquer si la r√©ponse est fond√©e / √©tay√©e par un ensemble de faits. \n
            Fournis le score binaire au format JSON avec une cl√© unique ¬´ score ¬ª et sans pr√©ambule ni explication.""",
            input_variables=["generation", "documents"],
        )

        hallucination_grader = prompt | llm | JsonOutputParser()
        hallucination_grader.invoke({"documents": docs, "generation": generation})

        ### Answer Grader
        llm = ChatOllama(
            model="mistral:latest",
            format="json",
            temperature=0,
        )

        prompt = PromptTemplate(
            template="""Tu es un √©valuateur qui √©value si une r√©ponse est utile pour r√©soudre une question. \n 
            Voici la r√©ponse :
            \n ------- \n
            {generation} 
            \n ------- \n
            Voici la question : {question}
            Donne un score binaire ¬´ oui ¬ª ou ¬´ non ¬ª pour indiquer si la r√©ponse est utile pour r√©soudre une question. \n
            Fournis le score binaire au format JSON avec une cl√© unique ¬´ score ¬ª et sans pr√©ambule ni explication.""",
            input_variables=["generation", "question"],
        )

        answer_grader = prompt | llm | JsonOutputParser()
        answer_grader.invoke({"question": question, "generation": generation})

        ### Question Re-writer
        llm = ChatOllama(
            model="mistral:latest",
            temperature=0,
        )

        re_write_prompt = PromptTemplate(
            template="""Tu es un reformulateur de questions qui convertit une question d'entr√©e en une meilleure version optimis√©e \n 
                pour la r√©cup√©ration du magasin vectoriel. Regarde la question initiale et formule une question am√©lior√©e. \n
                Tu devras obligatoirement r√©pondre en fran√ßais. \n
                Voici la question initiale : \n\n {question}. Question am√©lior√©e sans pr√©ambule : \n""",
            input_variables=["generation", "question"],
        )

        question_rewriter = re_write_prompt | llm | StrOutputParser()
        question_rewriter.invoke({"question": question})

        # 1Ô∏è‚É£ √âtape : R√©cup√©ration des documents
        def retrieve(question, retriever):
            log("---RETRIEVE---")
            documents = retriever.invoke(question)
            for document in documents:
                log(f'  üìÉ Document : {document.page_content}')
            return documents

        # 2Ô∏è‚É£ √âtape : Filtrage des documents
        def grade_documents(question, documents, retrieval_grader):
            log("---CHECK RELEVANCE---")
            filtered_docs = []
            for d in documents:
                score = retrieval_grader.invoke({"question": question, "document": d.page_content})
                if score["score"] == "oui":
                    log(f'  ‚úÖ Relevant : {d.page_content}')
                    filtered_docs.append(d)
                else:
                    log(f'  ‚ùå Not relevant : {d.page_content}')
            return filtered_docs

        # 3Ô∏è‚É£ √âtape : R√©√©criture de la question si n√©cessaire
        def transform_query(question, question_rewriter):
            log("---TRANSFORM QUERY---")
            better_question = question_rewriter.invoke({"question": question})
            log(f'üì© Nouvelle question : {better_question}')
            return better_question

        # 4Ô∏è‚É£ √âtape : Generation de la r√©ponse avec le mod√®le LLM
        def generate(question, documents, rag_chain):
            log("---GENERATE---")
            generation = rag_chain.invoke({"documents": documents, "question": question})
            log(f'üì© R√©ponse : {generation}')
            return generation

        # 5Ô∏è‚É£ √âtape : V√©rification de la r√©ponse (hallucination et pertinence)
        def validate_answer(question, documents, generation, hallucination_grader, answer_grader):
            log("---CHECK HALLUCINATIONS---")
            score = hallucination_grader.invoke({"documents": documents, "generation": generation})
    
            if score["score"] != "oui":
                log(f'‚ùå Hallucination d√©tect√©e')
                return False
            log(f"‚úÖ Pas d'hallucination!")

            log("---CHECK ANSWER---")
            score = answer_grader.invoke({"question": question, "generation": generation})
    
            if score["score"] != "oui":
                log(f'‚ùå R√©ponse incorrecte')
                return False
    
            log(f'‚úÖ R√©ponse valid√©e!')
            return True  # R√©ponse valid√©e

        # Fonction principale qui ex√©cute toutes les √©tapes
        def rag_pipeline(question, retriever, rag_chain, retrieval_grader, question_rewriter, hallucination_grader, answer_grader):
            max_attempts = 3  # Nombre maximum de reformulations
            attempt = 0  # Compteur de tentatives

            while attempt < max_attempts:
                # √âtape 1 : R√©cup√©ration
                documents = retrieve(question, retriever)

                # √âtape 2 : Filtrage des documents
                filtered_docs = grade_documents(question, documents, retrieval_grader)

                if not filtered_docs:
                    # Aucun document pertinent ‚Üí reformuler la question
                    attempt += 1
                    if attempt < max_attempts:
                        question = transform_query(question, question_rewriter)
                        continue  # Recommencer avec la question reformul√©e
                    else:
                        # Si on atteint la limite de tentatives
                        return "üòû Oups, nous n'avons pas trouv√© l'information souhait√©e! Tentez de reformuler votre question pour de meilleurs r√©sultats."

                # √âtape 3 : Generation de la r√©ponse
                generation = generate(question, filtered_docs, rag_chain)

                # √âtape 4 : V√©rification de la r√©ponse
                if validate_answer(question, filtered_docs, generation, hallucination_grader, answer_grader):
                    return generation  # R√©ponse valid√©e, on sort de la boucle
                else:
                    # Si la r√©ponse n'est pas fiable, reformuler la question
                    attempt += 1
                    if attempt < max_attempts:
                        question = transform_query(question, question_rewriter)
                    else:
                        return "üòû Oups, nous n'avons pas trouv√© l'information souhait√©e! Tentez de reformuler votre question pour de meilleurs r√©sultats."
        
        print("‚úÖ RAG charg√©.")

        return rag_pipeline, retriever, rag_chain, retrieval_grader, question_rewriter, hallucination_grader, answer_grader
    else:
        # Si la base vectorielle n'existe pas
        print("‚ùå Aucun VectorStore existant, cr√©ez en un avec generateStore.")
        return None
